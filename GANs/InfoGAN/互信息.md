## 互信息

在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是**变量间相互依赖性的量度**。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 $p(X,Y) $ 和分解的边缘分布的乘积 $p(X) $、$p(Y) $ 的相似程度。它是度量两个事件集合之间的相关性(mutual dependence)，最常用的单位是bit。

### 定义

对于两个离散的随机变量$X$ 和$Y$ ，它们的互信息定义如下：
$$
I(X,Y) = \sum_{y\in Y}\sum_{x \in X} p(x,y)\log (\frac{p(x,y)}{p(x)p(y)})
$$
其中$p(X,Y)$ 是关于$X$ 和$Y$ 的联合概率分布函数，$p(x)$和$p(y)$ 分别是关于$X$和$Y$的边缘概率分布函数。

对于两个连续的随机变量，它们的互信息定义如下：
$$
I(X,Y) = \int _{Y} \int_{X}p(x,y)\log (\frac{p(x,y)}{p(x)p(y)})dxdy
$$
直观上可以将其理解为两个变量$X$和$Y$之间共享信息多少的度量，如果互信息越大，我们知道其中一个变量的信息，对于另一个变量知道的信息就越多；反之互信息越小，彼此之间提供的信息就会越少。

### 互信息和其他度量的关系

![img](https://img-blog.csdn.net/20160718112932230)

互信息可以表示为如下的形式：
$$
I(X;Y)=H(X,Y)-H(X|Y)-H(Y|X)
$$
其中$H(X)$和$H(Y)$ 是边缘熵，$H(X|Y)$ 和$H(Y|X)$ 是条件熵，$H(X,Y)$ 是$X$和$Y$的联合熵。它们的关系用韦恩图表示如上所示。